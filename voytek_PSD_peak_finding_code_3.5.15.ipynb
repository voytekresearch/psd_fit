{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSD Peak Detection through Cross Validation and K-Means Clustering\n",
    "\n",
    "This notebook contains the code to extract peak parameters from PSDs of electrodes of a single subject. It is organized in the following manner :\n",
    "\n",
    "* Import necessary python libraries and load data\n",
    "* Define helper functions for finding peaks in PSDs\n",
    "* Define helper functions for k-means clustering\n",
    "* Define helper functions for bootstrapping\n",
    "* Cross validate one electrode of data\n",
    "* Return final peak center frequencies and bandwidths\n",
    "\n",
    "Each of the helper functions also has a docstring with a short description and summary of input arguments and output results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Import Libraries \n",
    "Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import cross_validation\n",
    "from sklearn import cluster\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Process_Elec_Data\n",
    "Given trials x time for a particular electrode, calculates the mean PSD, mean flatspec, and frequency vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_elec_data(elec_data, srate):\n",
    "    \"\"\"\n",
    "    Returns the f, mean, (mean - 1/f^2) of trials in elec_data\n",
    "\n",
    "    Args:\n",
    "      elec_data: Matrix of electrode data (trials x time)\n",
    "\n",
    "    Returns:\n",
    "      f: vector of frequencies (spacing decided with signal.welch)\n",
    "      psd_mean: average psd of all trials in elec_data matrix\n",
    "      flatspec: psd_mean - (1/f^2)\n",
    "\n",
    "    \"\"\"\n",
    "    # Resample\n",
    "    newsrate = 256 # new sampling rate\n",
    "    if srate!=newsrate:\n",
    "        elec_data = sp.signal.resample(elec_data, int(np.floor(len(elec_data)*(newsrate/srate))))\n",
    "  \n",
    "    # Calculate trial and mean PSDs for testing range of trials\n",
    "    f,psd_trial = sp.signal.welch(elec_data, fs=newsrate, window='hanning', nperseg=newsrate, noverlap=newsrate/2.0, nfft=None, detrend='linear', return_onesided=True, scaling='density')\n",
    "    psd_mean = psd_trial.mean(0)\n",
    "    \n",
    "    #Cut PSD according to frequency range of interest (1-50Hz)\n",
    "    f = f[1:50]\n",
    "    psd_mean = np.log10(psd_mean[1:50])\n",
    "    \n",
    "    #Calculate Flatspec (PSDMean - 1/f^2)\n",
    "    slope_params = np.polyfit(f, psd_mean, 2)\n",
    "    slopefit = np.polyval(slope_params, f)\n",
    "    flatspec = psd_mean - slopefit\n",
    "    \n",
    "    return (f,psd_mean, flatspec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Import and check data'''\n",
    "\n",
    "datafolder = 'INSERT PATH TO DATA HERE' # path to EEG_1elec_epoched_data.mat\n",
    "fname = 'EEG_1elec_epoched_data.mat';\n",
    "os.chdir(datafolder);\n",
    "data = sp.io.loadmat(fname);\n",
    "data = squeeze(data['elec_dat']);\n",
    "fx,pow_spec,flatspec = process_elec_data(data,256);\n",
    "plot(fx,pow_spec)\n",
    "ylabel('Power')\n",
    "xlabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Helper Functions for to Find Peaks and Fit Gaussians\n",
    "The following functions are used to identify peaks (```find_peaks```), create a combination of Gaussians given necessary parameters (```norm```), and find the best fit of Gaussians to a given function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Norm\n",
    "Norm takes in a vector of frequencies (essentially the x-axis) and a vector of parameters (arg) that characterizes the Gaussians. It uses these parameters to create a combination of Gaussians and returns it as fitdat.\n",
    "For example, if the following peaks were found:\n",
    "\n",
    "| Peaks | Amplitude | Std Dev |\n",
    "|------|------|----|\n",
    "|   7.0  | 0.05| 1.35|\n",
    "|   22.0  | 0.13| 0.66|\n",
    "\n",
    "then the arg vector would be \n",
    "```python\n",
    "[7.0 22.0 0.05 0.13 1.35 0.66 2]\n",
    "```\n",
    "where the last 2 stands for the total number of peaks found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def norm(x, *args):\n",
    "    \"\"\"\n",
    "    Calculates a combination of Gaussians given parameters\n",
    "    \n",
    "    Args:\n",
    "      x: vector of frequencies\n",
    "      *args: vector of Gaussian parameters (Gaussian center, Gaussian standard deviation, Gaussian height, # of Gaussians)\n",
    "\n",
    "    Returns:\n",
    "      fitdat: combination of Gaussians\n",
    "\n",
    "    \"\"\"\n",
    "    number_of_gaussians = int(args[-1]) # number of gaussians from peak detection\n",
    "    fitdat = np.array(0)\n",
    "    for peaki in range(number_of_gaussians):\n",
    "        fitdat = fitdat + args[peaki+(2*number_of_gaussians)]*sp.stats.norm.pdf(x, loc=args[peaki], scale=args[peaki+number_of_gaussians]) # add a bunch of gaussians together\n",
    "    return fitdat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Find_Peaks\n",
    "Uses Python's built in ```sp.signal.find_peaks_cwt ``` in order to find peaks using the given sliding window on the given flatspec (PSD - $\\frac{1}{f^2}$). The first window starts at 0Hz and looks for a peak between 0Hz and ```sliding_window```Hz. The next window starts at 2Hz and goes till ```sliding_window```+2Hz. This process continues till the entire freequency range has been covered.\n",
    "\n",
    "A sliding window is used to ensure that low amplitude peaks are not ignored. The 2Hz overlap ensures that peaks at edges of the sliding windows aren't missed [MAKE CLEAR]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from numpy import NaN, Inf, arange, isscalar, asarray, array\n",
    " \n",
    "    \n",
    "def peakdect(v, delta, x = None):\n",
    "    \"\"\"\n",
    "    Converted from MATLAB script at http://billauer.co.il/peakdet.html\n",
    "    \n",
    "    Returns two arraysX\n",
    "    \n",
    "    function [maxtab, mintab]=peakdet(v, delta, x)\n",
    "    %PEAKDET Detect peaks in a vector\n",
    "    %        [MAXTAB, MINTAB] = PEAKDET(V, DELTA) finds the local\n",
    "    %        maxima and minima (\"peaks\") in the vector V.\n",
    "    %        MAXTAB and MINTAB consists of two columns. Column 1\n",
    "    %        contains indices in V, and column 2 the found values.\n",
    "    %      \n",
    "    %        With [MAXTAB, MINTAB] = PEAKDET(V, DELTA, X) the indices\n",
    "    %        in MAXTAB and MINTAB are replaced with the corresponding\n",
    "    %        X-values.\n",
    "    %\n",
    "    %        A point is considered a maximum peak if it has the maximal\n",
    "    %        value, and was preceded (to the left) by a value lower by\n",
    "    %        DELTA.\n",
    "    \n",
    "    % Eli Billauer, 3.4.05 (Explicitly not copyrighted).\n",
    "    % This function is released to the public domain; Any use is allowed.\n",
    "    \n",
    "    \"\"\"\n",
    "    maxtab = []\n",
    "    mintab = []\n",
    "       \n",
    "    if x is None:\n",
    "        x = arange(len(v))\n",
    "    \n",
    "    v = asarray(v)\n",
    "    \n",
    "    if len(v) != len(x):\n",
    "        sys.exit('Input vectors v and x must have same length')\n",
    "    \n",
    "    if not isscalar(delta):\n",
    "        sys.exit('Input argument delta must be a scalar')\n",
    "    \n",
    "    if delta <= 0:\n",
    "        sys.exit('Input argument delta must be positive')\n",
    "    \n",
    "    mn, mx = Inf, -Inf\n",
    "    mnpos, mxpos = NaN, NaN\n",
    "    \n",
    "    lookformax = True\n",
    "    \n",
    "    for i in arange(len(v)):\n",
    "        this = v[i]\n",
    "        if this > mx:\n",
    "            mx = this\n",
    "            mxpos = x[i]\n",
    "        if this < mn:\n",
    "            mn = this\n",
    "            mnpos = x[i]\n",
    "        \n",
    "        if lookformax:\n",
    "            if this < mx-delta:\n",
    "                maxtab.append((mxpos, mx))\n",
    "                mn = this\n",
    "                mnpos = x[i]\n",
    "                lookformax = False\n",
    "        else:\n",
    "            if this > mn+delta:\n",
    "                mintab.append((mnpos, mn))\n",
    "                mx = this\n",
    "                mxpos = x[i]\n",
    "                lookformax = True\n",
    " \n",
    "    return array(maxtab), array(mintab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_peaks (flatspec, sliding_window, f):\n",
    "    \"\"\"\n",
    "    Detects peaks in flatspec in in chunks of width sliding_window. A window overlaps previous window by 2Hz\n",
    "    \n",
    "    Args:\n",
    "      flatspec: psd_mean - (1/f^2)\n",
    "      f: frequency of vectors\n",
    "      sliding_window: width of sliding window for detecting peaks\n",
    "\n",
    "    Returns:\n",
    "      peaks: indices of peak locations in flatspec\n",
    "    \"\"\"\n",
    "    maxpks, minpks = peakdect(flatspec, sliding_window, f) # find peaks in sliding window range\n",
    "    try:\t\n",
    "    \tpeakind = [np.uint32(i) for i in (array(maxpks)[:,0]) if i <= 30] #ignore peaks beyond 30Hz\n",
    "    except Exception:\n",
    "\tpeakind = [-1]\n",
    "    try:\n",
    "    \tvalleyind = [np.uint32(i) for i in (array(minpks)[:,0]) if i <= 30] #ignore peaks beyond 30Hz\n",
    "    except Exception:\n",
    "\tvalleyind = [-1]\n",
    "    return peakind, valleyind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Transform_Data\n",
    "Takes list of all parameters from CV and converts into form for Kmeans clustering and separates peaks from sliding windows for plotting. Currently allowing for a maximum of three sliding windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_data (params):\n",
    "    \"\"\"\n",
    "    Converts list of lists to matrix in form (n_components x n_features)\n",
    "\n",
    "    Args:\n",
    "      params: parameters from CV folds\n",
    "\n",
    "    Returns:\n",
    "      X: matrix of parameters\n",
    "\n",
    "    \"\"\"\n",
    "    #separate peaks for each sliding window width\n",
    "    peaks_np = np.array(params)\n",
    "    #separate peaks for each sliding window width\n",
    "    peaks1 = [elem for sublist in map(list, peaks_np[:,0,:]) for elem in sublist if elem >=0] #only include peaks less than 30Hz\n",
    "    peaks2 = [elem for sublist in map(list, peaks_np[:,1,:]) for elem in sublist if elem >=0] #only include peaks less than 30Hz\n",
    "    peaks3 = [elem for sublist in map(list, peaks_np[:,2,:]) for elem in sublist if elem >=0] #only include peaks less than 30Hz]\n",
    "\n",
    "    #combine all peaks for input into Kmeans clustering algorithm\n",
    "    peaks_comp = np.array(peaks1+peaks2+peaks3)\n",
    "    peaks_comp = [elem for elem in peaks_comp if (elem<30 and elem >= 0)] #only include peaks less than 30Hz\n",
    "    X = np.reshape(peaks_comp, (1,len(peaks_comp)))\n",
    "    X=X.transpose()\n",
    "    return X, peaks1, peaks2, peaks3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###KMeans\n",
    "Uses ```scikit.learn.cluster.KMeans``` to find ```cluster_size``` clusters among all peaks from all CV folds and all sliding window widths (in ```X```). \n",
    "Returns cluster centers and sum of squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kmeans(X, cluster_size):\n",
    "    \"\"\"\n",
    "    Returns the cluster centroids and SoS from centroids\n",
    "\n",
    "    Args:\n",
    "      X: Data (n_componentsxn_features) to cluster\n",
    "\n",
    "    Returns:\n",
    "      clusters: cluster centroids\n",
    "      inertia: SoS\n",
    "\n",
    "    \"\"\"\n",
    "    km = cluster.KMeans(n_clusters=cluster_size)\n",
    "    km.fit(X)\n",
    "    clusters = km.cluster_centers_\n",
    "    return clusters, km.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Find_Clusters\n",
    "Uses the list of sum of squares for each cluster size to find \"knee\" in fit. Also takes into consideration the change in SoS after the knee in order to account for missed cluster centers. [REWRITE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_cluster(X):\n",
    "    \"\"\"\n",
    "    Finds ideal number of clusters\n",
    "    \n",
    "    Args:\n",
    "        peaks: list of all peaks\n",
    "    Returns:\n",
    "        ideal number of clusters\n",
    "    \"\"\"\n",
    "    SoS_list = []\n",
    "    peaks = X[:,0:1]\n",
    "    med = np.median(peaks)\n",
    "    zeroSoS = sum(((x-med)*(x-med) for x in peaks))\n",
    "    SoS_list.append(zeroSoS)\n",
    "    for cluster_size in range (1, 6):\n",
    "        p,SoS_tmp = kmeans(X, cluster_size)\n",
    "        SoS_list.append((SoS_tmp))\n",
    "        \n",
    "    dx = np.array(SoS_list[:-1]) - np.array(SoS_list[1:]) #calculate difference in SoS\n",
    "    SoS_knee = np.argmax(dx) #pick max change\n",
    "    return SoS_knee+2, SoS_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Bootstrap code to calculate final values and confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bootstrap(data):\n",
    "    bootstrap_dist = np.zeros(1000)\n",
    "    for i in range(0,1000):\n",
    "        total_points = len(data)\n",
    "        data_sample = np.random.choice(np.array(data), size=int(0.8*total_points))\n",
    "        bootstrap_dist[i] = median(data_sample)\n",
    "    return [median(bootstrap_dist), percentile(bootstrap_dist, 2.5), percentile(bootstrap_dist, 97.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def final_check_for_output(extcv_pks,extcv_std, flatspec):\n",
    "    extcv_pks_raster = []\n",
    "    extcv_std_raster = []\n",
    "    \n",
    "    final_peaks = [];\n",
    "    final_stds = [];\n",
    "    bin_thresh = 0.05;\n",
    "    \n",
    "    for cv in range(len(extcv_pks)):\n",
    "        for i in range(len(extcv_pks[cv])):\n",
    "            if (extcv_pks[cv][i] <=30 and extcv_pks[cv][i] >=0):\n",
    "                if (flatspec[extcv_pks[cv][i]] > 0): \n",
    "                    extcv_pks_raster.append(extcv_pks[cv][i])\n",
    "                    extcv_std_raster.append(extcv_std[cv][i])\n",
    "\n",
    "    #Build Histogram of External CV Peaks\n",
    "    bin_count, bin_edge = np.histogram(extcv_pks_raster, 5)\n",
    "    valid_peak_range = []\n",
    "    for i in range(len(bin_count)):\n",
    "        if (bin_count[i]) >= bin_thresh*len(extcv_pks_raster):\n",
    "            valid_peak_range.append([bin_edge[i], bin_edge[i+1]])\n",
    "\n",
    "    #Place Peaks and other Params in Appropriate Bins    \n",
    "    sorted_params = []\n",
    "    for low, high in valid_peak_range:\n",
    "        temp_ind = [i for i in range(len(extcv_pks_raster)) if low<=extcv_pks_raster[i]<=high]\n",
    "        sorted_params.append([np.array(extcv_pks_raster)[temp_ind], np.array(extcv_std_raster)[temp_ind]])\n",
    "    \n",
    "    for j in range(len(valid_peak_range)):\n",
    "        med_peak = median(sorted_params[j],1)[0]\n",
    "        med_std = median(sorted_params[j],1)[1]\n",
    "        if med_std <=10:\n",
    "            cf_params = bootstrap(sorted_params[j][0])\n",
    "            bw_params = bootstrap(2.355*sorted_params[j][1])\n",
    "            final_peaks.append(cf_params[0])\n",
    "            final_stds.append(bw_params[0])\n",
    "    \n",
    "    return final_peaks, final_stds\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Peak Fitting Function\n",
    "Calls above functions and performs multiple fits on peak locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def final_peak_fitting(elec_data,srate):\n",
    "    \n",
    "    '''\n",
    "    Uses cross validation to calculate the PSD and fit gaussians to electrophysiological data\n",
    "    \n",
    "    Args:\n",
    "         data: trials x time\n",
    "         srate: sampling rate of the data \n",
    "         extcv: range of cross validation runs\n",
    "         \n",
    "    Returns:\n",
    "        extcv_peaks = Center frequencies of all peaks\n",
    "        extcv_std = Standard deviation for each peak found\n",
    "    '''   \n",
    "\n",
    "    freq_vector, psd_mean, flatspec = process_elec_data(elec_data, srate)\n",
    "    \n",
    "    extcv = range(0,10)\n",
    "    deltas = [0.03, 0.05, 0.07] \n",
    "    extcv_amps = []\n",
    "    extcv_rsq = []\n",
    "    extcv_std = []\n",
    "    extcv_pks = []\n",
    "    extcv_fit = []   \n",
    "    extcv_trial = []\n",
    "    extcv_valid = []\n",
    "    extcv_cluster_peaks = []\n",
    "    extcv_params = []\n",
    "    extcv_rscore=[]\n",
    "    extcv_delta_pks=[]\n",
    "    \n",
    "    for i in extcv:\n",
    "\t\t\n",
    "\t\t\"\"\"Perform Cross Validation\"\"\"\n",
    "\t\t#Set aside validation set\n",
    "\t\telec_data_train, elec_data_valid = cross_validation.train_test_split(elec_data, test_size=0.2, random_state=np.uint(np.random.rand(1)*100)[0]) #split into test and validation set #split into test and validation set\n",
    "\t\t\t\n",
    "\t\t#Split train set into 100 train and test sets\n",
    "\t\tss = cross_validation.ShuffleSplit(int(np.size(elec_data_train,0)*0.8), n_iter=100, test_size=0.2, random_state=0)\n",
    "\t\t\n",
    "\t\t#Use each window width on each of the 100 folds for fitting Gaussians\n",
    "\t\tpeaks_cv=[]\n",
    "\t\tvalleys_cv=[]\n",
    "\t\tfor train_index, test_index in ss:\n",
    "\t\t\t#split into train and test set\n",
    "\t\t\ttrials_train = elec_data_train[train_index,:]\n",
    "\t\t\ttrials_test = elec_data_train[test_index,:]\n",
    "\t\t\tfreq_vector, _, flatspec_train = process_elec_data(trials_train, srate)  #calculate train flatspec\n",
    "\t\t\t_, _, flatspec_test = process_elec_data(trials_test, srate)  #calculate test flatspec\n",
    "\t\t\tpeaks_cv_temp = np.zeros((3,12))\n",
    "\t\t\tvalleys_cv_temp = np.zeros((3,12))\n",
    "\t\t\t#find Gaussian fit parameters for each window width\n",
    "\t\t\tfor d in range(0,len(deltas)):\n",
    "\t\t\t\tdelta = deltas[d]\n",
    "\t\t\t\tpeaks_temp, valleys_temp = find_peaks(flatspec_train, delta, freq_vector)\n",
    "\t\t\t\tpeaks_temp_vect = np.zeros(12)\n",
    "\t\t\t\tvalleys_temp_vect = np.zeros(12)\n",
    "\t\t\t\tpeaks_temp_vect[0:len(peaks_temp)] = peaks_temp\n",
    "\t\t\t\tvalleys_temp_vect[0:len(valleys_temp)] = valleys_temp\n",
    "\t\t\t\tpeaks_cv_temp[d] = peaks_temp_vect\n",
    "\t\t\t\tvalleys_cv_temp[d] = valleys_temp_vect\n",
    "\t\t\tpeaks_cv.append((peaks_cv_temp))\n",
    "\t\t\tvalleys_cv.append((valleys_cv_temp))\n",
    "\t\t\n",
    "\t\textcv_delta_pks.append((peaks_cv))\n",
    "\t\t#Reshape Gaussian parameters to k-means clustering input format\n",
    "\t\tX_peaks, peaks1, peaks2, peaks3 = transform_data(peaks_cv)\n",
    "\t\tpeaks = X_peaks[:,0:1]\n",
    "\t\t\n",
    "\t\tX_valleys, valleys1, valleys2, valleys3 = transform_data(valleys_cv)\n",
    "\t\tvalleys = X_valleys[:,0:1]\n",
    "\t\t\n",
    "\t\t#Find ideal number of clusters, return all SoS for plotting\n",
    "\t\tcluster_num_peaks, cluster_SoS_peaks = find_cluster(X_peaks) \n",
    "\t\tcluster_num_valleys, cluster_SoS_valleys = find_cluster(X_valleys) \n",
    "\t\t\n",
    "\t\t#process valid and train sets\n",
    "\t\tf, mean_train, flatspec_train = process_elec_data(elec_data_train, srate)\n",
    "\t\t_, mean_valid, flatspec_valid = process_elec_data(elec_data_valid, srate)\n",
    "\t\textcv_trial.append((flatspec_train))\n",
    "\t\textcv_valid.append((flatspec_valid))\n",
    "\t\t\n",
    "\t\t#use ideal number of clusters to generate best fit\n",
    "\t\tcluster_peaks,_ = kmeans(X_peaks,cluster_num_peaks)\n",
    "\t\tcluster_valleys,_ = kmeans(X_valleys,cluster_num_valleys)\n",
    "\t\tpeak = [elem for sublist in map(list, np.array(cluster_peaks)) for elem in sublist]\n",
    "\t\tvalley = [elem for sublist in map(list, np.array(cluster_valleys)) for elem in sublist]\n",
    "\t\tcluster_all = peak+valley\n",
    "\t\tpeakind = np.uint32([p for p in cluster_all if p>=1 and p<=30])\n",
    "\t\tp = np.linspace(1, len(peakind), len(peakind))\n",
    "\t\tparams = list(freq_vector[peakind]) + list(p.T) + list(flatspec_train[peakind]*2) + [len(peakind)] # initial guess for parameters based on peak detection\n",
    "\t\ttry:\n",
    "\t\t\toscillatory_fit_params,_ = sp.optimize.curve_fit(norm, f, flatspec_train, p0=params) # fitting parameters\n",
    "\t\texcept RuntimeError:\n",
    "\t\t\tprint('FAILED FIT')\n",
    "\t\t\tcontinue\n",
    "\t\t\t\n",
    "\t\tcluster_fit = norm(f, *oscillatory_fit_params)\n",
    "\t\tcluster_num = oscillatory_fit_params[-1]\n",
    "\t\textcv_std.append((oscillatory_fit_params[cluster_num:2*cluster_num]))\n",
    "\t\textcv_pks.append((oscillatory_fit_params[0:cluster_num]))\n",
    "\n",
    "    final_peaks, final_bws = final_clustering_for_output(extcv_pks,extcv_std, flatspec);\n",
    "\n",
    "    return final_peaks, final_bws\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "peaks,bws = final_peak_fitting(data,256);\n",
    "\n",
    "print peaks, bws"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
